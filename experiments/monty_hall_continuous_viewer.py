#!/usr/bin/env python3
"""
Visualiseur continu d'Ã©pisodes Monty Hall - Permet de voir plusieurs Ã©pisodes avec contrÃ´les de vitesse.
"""

import sys
import os
import time
import numpy as np

# Ajouter le rÃ©pertoire parent au path pour importer les modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from environments.monty_hall import MontyHall
from environments.visualization.monty_hall_visualizer import MontyHallVisualizer
from algorithms.dynamic_programming.value_iteration import ValueIteration

def train_agent():
    """EntraÃ®ne un agent Value Iteration."""
    print("EntraÃ®nement de l'agent...")
    
    env = MontyHall()
    agent = ValueIteration(
        environment=env,
        discount_factor=0.9,
        theta=0.001,
        max_iterations=1000
    )
    
    results = agent.train()
    print(f"Agent entraÃ®nÃ© en {results['iterations']} itÃ©rations!")
    
    return agent

def run_continuous_episodes(env, visualizer, agent, num_episodes=20):
    """ExÃ©cute plusieurs Ã©pisodes en continu avec contrÃ´les de vitesse."""
    print(f"=== VISUALISATION CONTINUE - {num_episodes} Ã‰PISODES ===")
    print("ContrÃ´les:")
    print("- ESPACE: Pause/Reprendre")
    print("- + : Augmenter la vitesse")
    print("- - : Diminuer la vitesse")
    print("- Ã‰CHAP: Quitter")
    
    # Variables de contrÃ´le
    paused = False
    speed = 1.0  # DÃ©lai entre les Ã©tapes (secondes)
    min_speed = 0.1
    max_speed = 3.0
    
    # Statistiques
    wins = 0
    total_reward = 0
    
    for episode in range(1, num_episodes + 1):
        state = env.reset()
        episode_reward = 0
        steps = 0
        actions_taken = []
        
        while True:
            # Afficher l'Ã©tat actuel
            episode_info = {
                "Ã‰pisode": f"{episode}/{num_episodes}",
                "Mode": "Agent entraÃ®nÃ©",
                "Ã‰tapes": steps,
                "RÃ©compense": episode_reward,
                "Actions": actions_taken,
                "Vitesse": f"{speed:.1f}x",
                "Pause": "PAUSE" if paused else "EN COURS",
                "Statistiques": f"GagnÃ©s: {wins}/{episode-1} ({wins/max(episode-1,1)*100:.1f}%)"
            }
            visualizer.render_monty_hall_state(env, episode_info)
            
            # GÃ©rer les Ã©vÃ©nements
            event = visualizer.handle_events()
            if event == "pause":
                paused = not paused
            elif not visualizer.running:
                return wins, episode - 1
            
            if paused:
                continue
            
            # Choisir l'action
            if env.state == 0:
                # Choix initial de porte
                action = np.random.choice([0, 1, 2])
            else:
                # Action rester/changer selon la politique
                action = np.argmax(agent.policy[state])
            
            actions_taken.append(action)
            
            # ExÃ©cuter l'action
            state, reward, done, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            # DÃ©lai selon la vitesse
            time.sleep(speed)
            
            if done:
                # Mettre Ã  jour les statistiques
                if info.get("result") == "win":
                    wins += 1
                total_reward += episode_reward
                
                # Afficher le rÃ©sultat final de l'Ã©pisode
                result = "GAGNÃ‰! ğŸ‰" if info.get("result") == "win" else "PERDU! âŒ"
                episode_info = {
                    "Ã‰pisode": f"{episode}/{num_episodes}",
                    "Mode": "Agent entraÃ®nÃ©",
                    "Ã‰tapes": steps,
                    "RÃ©compense": episode_reward,
                    "Actions": actions_taken,
                    "RÃ©sultat": result,
                    "Statistiques": f"GagnÃ©s: {wins}/{episode} ({wins/episode*100:.1f}%)"
                }
                visualizer.render_monty_hall_state(env, episode_info)
                
                # Pause courte pour voir le rÃ©sultat
                time.sleep(0.5)
                break
    
    return wins, num_episodes

def main_continuous_viewer():
    """Visualiseur continu principal."""
    print("=== VISUALISEUR CONTINU MONTY HALL ===")
    print("Ce script permet de voir plusieurs Ã©pisodes en continu.")
    
    # CrÃ©er l'environnement et le visualiseur
    env = MontyHall()
    visualizer = MontyHallVisualizer()
    
    # EntraÃ®ner l'agent
    agent = train_agent()
    
    # Afficher la politique de l'agent
    print(f"\nPolitique de l'agent:")
    for state in range(len(env.S)):
        if state not in env.T:
            action = np.argmax(agent.policy[state])
            action_name = "Changer" if action == 1 else "Garder"
            value = agent.V[state]
            print(f"Ã‰tat {state}: {action_name} (V(s) = {value:.3f})")
    
    # ExÃ©cuter les Ã©pisodes continus
    wins, total = run_continuous_episodes(env, visualizer, agent, num_episodes=20)
    
    # RÃ©sultats finaux
    if total > 0:
        win_rate = wins / total
        print(f"\n=== RÃ‰SULTATS FINAUX ===")
        print(f"Ã‰pisodes gagnÃ©s: {wins}/{total}")
        print(f"Taux de rÃ©ussite: {win_rate:.2%}")
        
        # Comparaison avec la thÃ©orie
        expected_rate = 2/3  # ThÃ©oriquement, changer donne 2/3 de chance de gagner
        print(f"Taux thÃ©orique (changer): {expected_rate:.2%}")
        
        if win_rate > expected_rate * 0.9:
            print("L'agent performe bien par rapport Ã  la thÃ©orie!")
        else:
            print("L'agent pourrait mieux performer.")
    
    # Attendre que l'utilisateur ferme la fenÃªtre
    print("\nAppuyez sur Ã‰CHAP ou fermez la fenÃªtre pour quitter...")
    while visualizer.running:
        event = visualizer.handle_events()
        if event is not None:
            break
        time.sleep(0.1)
    
    visualizer.quit()

if __name__ == "__main__":
    main_continuous_viewer() 