"""
Line World Environment
Sp√©cifications:
- √âtats: [0, 1, 2, 3, 4]
- Actions: [0, 1] (0: Left, 1: Right)  
- R√©compenses: [-1.0, 0.0, 1.0]
- √âtats terminaux: [0, 4]

Transitions d√©terministes:
- Position 1: Left ‚Üí 0 (reward -1), Right ‚Üí 2 (reward 0)
- Position 2: Left ‚Üí 1 (reward 0), Right ‚Üí 3 (reward 0)  
- Position 3: Left ‚Üí 2 (reward 0), Right ‚Üí 4 (reward 1)

Compatible avec TOUS les algorithmes RL :
- Algorithmes bas√©s exp√©rience : Q-Learning, SARSA, Monte Carlo
- Algorithmes bas√©s mod√®le : Policy Iteration, Value Iteration, Dynamic Programming
"""

import numpy as np
from typing import Tuple, List, Dict, Any

from src.rl_environments.base_environment import BaseEnvironment


class LineWorld(BaseEnvironment):
    """
    Environnement Line World selon les sp√©cifications du projet.
    
    M√©thode diff√©rente selon type de mod√®le, mais compatible avec tous les mod√®le :
    - Exp√©rience : Q-Learning, SARSA, Monte Carlo (via step/reset)
    - Mod√®le : Policy Iteration, Value Iteration (via matrices de transition)
    """
    
    # Actions
    ACTION_LEFT = 0
    ACTION_RIGHT = 1
    ACTION_NAMES = {ACTION_LEFT: "Left", ACTION_RIGHT: "Right"}
    
    REWARD_NEGATIVE = -1.0  # Position 0
    REWARD_NEUTRAL = 0.0    # D√©placements normaux
    REWARD_POSITIVE = 1.0   # Position 4
    
    def __init__(self, max_steps: int = 100):
        """
        Initialise LineWorld avec les r√®gles fixes du projet.
        
        Args:
            max_steps: Nombre maximum d'√©tapes par √©pisode
        """
        super().__init__("LineWorld")
        
        self.max_steps = max_steps
        self.steps_taken = 0
        
        # Configuration fixe selon les sp√©cifications
        self.states = [0, 1, 2, 3, 4]
        self.actions = [0, 1]  # Left, Right
        self.terminal_states = [0, 4]
        self.start_position = 2  # Position de d√©part fixe
        
        # Matrices pour les algorithmes bas√©s mod√®le
        self._setup_model_matrices()
        
        # Transitions pour les algorithmes bas√©s exp√©rience
        self._setup_transitions()
        
        self.current_state = None
        self.reset()
    
    def _setup_model_matrices(self):
        """Configure les matrices compl√®tes pour algorithmes bas√©s mod√®le."""
        num_states = len(self.states)
        num_actions = len(self.actions)
        
        # Matrice de transition P(s'|s,a) : [state][action][next_state]
        self.transition_matrix = np.zeros((num_states, num_actions, num_states))
        
        # Matrice de r√©compenses R(s,a,s') : [state][action][next_state]
        self.reward_matrix = np.zeros((num_states, num_actions, num_states))
        
        # Remplissage selon les transitions d√©finies
        # Position 1: Left ‚Üí 0 (reward -1), Right ‚Üí 2 (reward 0)
        self.transition_matrix[1, 0, 0] = 1.0
        self.reward_matrix[1, 0, 0] = self.REWARD_NEGATIVE
        
        self.transition_matrix[1, 1, 2] = 1.0
        self.reward_matrix[1, 1, 2] = self.REWARD_NEUTRAL
        
        # Position 2: Left ‚Üí 1 (reward 0), Right ‚Üí 3 (reward 0)
        self.transition_matrix[2, 0, 1] = 1.0
        self.reward_matrix[2, 0, 1] = self.REWARD_NEUTRAL
        
        self.transition_matrix[2, 1, 3] = 1.0
        self.reward_matrix[2, 1, 3] = self.REWARD_NEUTRAL
        
        # Position 3: Left ‚Üí 2 (reward 0), Right ‚Üí 4 (reward 1)
        self.transition_matrix[3, 0, 2] = 1.0
        self.reward_matrix[3, 0, 2] = self.REWARD_NEUTRAL
        
        self.transition_matrix[3, 1, 4] = 1.0
        self.reward_matrix[3, 1, 4] = self.REWARD_POSITIVE
        
        # √âtats terminaux (restent sur place)
        for terminal_state in self.terminal_states:
            for action in self.actions:
                self.transition_matrix[terminal_state, action, terminal_state] = 1.0
                self.reward_matrix[terminal_state, action, terminal_state] = 0.0
    
    def _setup_transitions(self):
        """Configure les transitions pour algorithmes bas√©s exp√©rience."""
        # Matrice de transition simple: [√©tat][action] = (next_state, reward)
        self.transitions = {
            1: {0: (0, self.REWARD_NEGATIVE), 1: (2, self.REWARD_NEUTRAL)},
            2: {0: (1, self.REWARD_NEUTRAL), 1: (3, self.REWARD_NEUTRAL)},
            3: {0: (2, self.REWARD_NEUTRAL), 1: (4, self.REWARD_POSITIVE)}
        }
    
    @property
    def state_space_size(self) -> int:
        """Retourne la taille de l'espace d'√©tats."""
        return len(self.states)
    
    @property
    def action_space_size(self) -> int:
        """Retourne la taille de l'espace d'actions."""
        return len(self.actions)
    
    @property
    def valid_actions(self) -> List[int]:
        """Retourne les actions valides dans l'√©tat actuel."""
        if self.current_state in self.terminal_states:
            return []
        return self.actions.copy()
    
    def reset(self) -> int:
        """
        Remet l'environnement √† l'√©tat initial.
        
        Returns:
            √âtat initial
        """
        self.current_state = self.start_position
        self.steps_taken = 0
        self._reset_episode_stats()
        return self.current_state
    
    def step(self, action: int) -> Tuple[int, float, bool, Dict[str, Any]]:
        """
        Ex√©cute une action selon les r√®gles du professeur.
        
        Args:
            action: 0 (Left) ou 1 (Right)
            
        Returns:
            (next_state, reward, done, info)
        """
        if action not in self.valid_actions:
            raise ValueError(f"Action invalide: {action}. Actions valides: {self.valid_actions}")
        
        if self.current_state in self.terminal_states:
            raise ValueError(f"√âpisode termin√©, impossible d'agir depuis l'√©tat {self.current_state}")
        
        self.steps_taken += 1
        old_state = self.current_state
        
        # Transition selon les r√®gles
        if self.current_state in self.transitions:
            next_state, reward = self.transitions[self.current_state][action]
        else:
            next_state, reward = self.current_state, 0.0
        
        self.current_state = next_state
        
        # V√©rification fin d'√©pisode
        done = (next_state in self.terminal_states) or (self.steps_taken >= self.max_steps)
        
        # Informations suppl√©mentaires
        info = {
            "action_name": self.ACTION_NAMES[action],
            "old_state": old_state,
            "terminal_reached": next_state in self.terminal_states,
            "max_steps_reached": self.steps_taken >= self.max_steps,
            "steps_taken": self.steps_taken,
            "target_reached": next_state == 4  # Pour compatibilit√© avec Agent
        }
        
        # Mise √† jour statistiques
        self._update_episode_stats(action, reward, next_state, done)
        
        return next_state, reward, done, info
    
    def render(self, mode: str = 'console'):
        """
        Affiche l'√©tat actuel de l'environnement.
        
        Args:
            mode: 'console' ou 'pygame'
        """
        if mode == 'console':
            self._render_console()
        elif mode == 'pygame':
            return self._render_pygame()
        else:
            raise ValueError(f"Mode non support√©: {mode}")
    
    def _render_console(self):
        """Affichage console simple et clair."""
        print(f"\n=== LineWorld (Step {self.steps_taken}) ===")
        
        # Ligne des positions
        position_line = "Pos: "
        visual_line = "     "
        
        for pos in self.states:
            position_line += f"{pos:2d} "
            
            if pos == self.current_state:
                if pos == 0:
                    visual_line += "[A]"  # Agent sur case perdante
                elif pos == 4:
                    visual_line += "[A]"  # Agent sur case gagnante  
                else:
                    visual_line += "[A]"  # Agent en position normale
            elif pos == 0:
                visual_line += "(-)"   # Case perdante
            elif pos == 4:
                visual_line += "(+)"   # Case gagnante
            else:
                visual_line += " . "   # Case normale
        
        print(position_line)
        print(visual_line)
        
        # √âtat et r√©compenses
        print(f"Agent en position: {self.current_state}")
        print(f"R√©compense totale: {self.total_reward:.1f}")
        
        # Actions disponibles
        if self.valid_actions:
            print("Actions: [0] Left ‚Üê | [1] Right ‚Üí")
        else:
            print("√âtat terminal - Aucune action possible")
    
    def _render_pygame(self):
        """Retourne les donn√©es pour rendu PyGame."""
        return {
            'positions': self.states,
            'current_position': self.current_state,
            'terminal_states': self.terminal_states,
            'steps_taken': self.steps_taken,
            'total_reward': self.total_reward,
            'valid_actions': self.valid_actions,
            'max_steps': self.max_steps
        }
    
    def get_state_description(self, state: int) -> str:
        """
        Description textuelle d'un √©tat.
        
        Args:
            state: √âtat √† d√©crire
            
        Returns:
            Description de l'√©tat
        """
        if state == 0:
            return f"Position {state} (TERMINAL - Perte)"
        elif state == 4:
            return f"Position {state} (TERMINAL - Victoire)"
        else:
            return f"Position {state} (Normal)"
    
    # ============ M√âTHODES POUR ALGORITHMES BAS√âS MOD√àLE ============
    
    def get_transition_matrix(self):
        """
        Retourne la matrice de transition compl√®te P(s'|s,a).
        
        Utilis√©e par : Policy Iteration, Value Iteration, Dynamic Programming
        
        Returns:
            np.ndarray: Matrice [state][action][next_state] = probabilit√©
        """
        return self.transition_matrix.copy()
    
    def get_reward_matrix(self):
        """
        Retourne la matrice de r√©compenses compl√®te R(s,a,s').
        
        Utilis√©e par : Policy Iteration, Value Iteration, Dynamic Programming
        
        Returns:
            np.ndarray: Matrice [state][action][next_state] = r√©compense
        """
        return self.reward_matrix.copy()
    
    def get_terminal_states(self):
        """
        Retourne les √©tats terminaux.
        
        Utilis√©e par : Tous les algorithmes bas√©s mod√®le
        
        Returns:
            List[int]: Liste des √©tats terminaux
        """
        return self.terminal_states.copy()
    
    def get_all_states(self):
        """Retourne tous les √©tats possibles."""
        return self.states.copy()
    
    def get_all_actions(self):
        """Retourne toutes les actions possibles."""
        return self.actions.copy()
    
    # ============ M√âTHODES POUR ALGORITHMES BAS√âS EXP√âRIENCE ============
    
    def get_optimal_policy(self) -> Dict[int, int]:
        """
        Retourne la politique optimale.
        
        Returns:
            Dict {state: optimal_action}
        """
        # Politique optimale: toujours aller vers la droite (position 4)
        return {
            1: self.ACTION_RIGHT,  # 1 ‚Üí 2
            2: self.ACTION_RIGHT,  # 2 ‚Üí 3  
            3: self.ACTION_RIGHT   # 3 ‚Üí 4 (victoire)
        }
    
    def get_reward_function(self, state: int, action: int, next_state: int) -> float:
        """
        Fonction de r√©compense.
        
        Args:
            state: √âtat de d√©part
            action: Action ex√©cut√©e  
            next_state: √âtat d'arriv√©e
            
        Returns:
            R√©compense de la transition
        """
        if state in self.transitions and action in self.transitions[state]:
            _, reward = self.transitions[state][action]
            return reward
        else:
            return 0.0
    
    def is_terminal(self, state: int) -> bool:
        """V√©rifie si un √©tat est terminal."""
        return state in self.terminal_states


# Fonction pour cr√©er l'environnement standard
def create_lineworld():
    """Cr√©e l'environnement LineWorld standard."""
    return LineWorld()


if __name__ == "__main__":
    # Test rapide
    print("üß™ Test LineWorld")
    env = create_lineworld()
    
    print("√âtat initial:")
    env.render()
    
    print("\nTest de quelques actions:")
    actions_test = [1, 1, 0, 1]  # Right, Right, Left, Right
    
    for i, action in enumerate(actions_test):
        if not env.valid_actions:
            print("√âpisode termin√©!")
            break
            
        print(f"\n--- Action {i+1}: {env.ACTION_NAMES[action]} ---")
        state, reward, done, info = env.step(action)
        print(f"R√©sultat: √âtat {state}, Reward {reward}")
        env.render()
        
        if done:
            print(f"√âpisode termin√©! Success: {info['target_reached']}")
            break
    
    # Test des matrices pour algorithmes bas√©s mod√®le
    print("\nüîç Test matrices pour Dynamic Programming:")
    print("Matrice de transition shape:", env.get_transition_matrix().shape)
    print("Matrice de r√©compenses shape:", env.get_reward_matrix().shape)
    print("√âtats terminaux:", env.get_terminal_states())
