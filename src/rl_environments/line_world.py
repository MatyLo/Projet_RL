"""
Line World Environment

SpÃ©cifications:
- Ã‰tats: [0, 1, 2, 3, 4]
- Actions: [0, 1] (0: Left, 1: Right)  
- RÃ©compenses: [-1.0, 0.0, 1.0]
- Ã‰tats terminaux: [0, 4]

Transitions dÃ©terministes:
- Position 1: Left â†’ 0 (reward -1), Right â†’ 2 (reward 0)
- Position 2: Left â†’ 1 (reward 0), Right â†’ 3 (reward 0)  
- Position 3: Left â†’ 2 (reward 0), Right â†’ 4 (reward 1)

Compatible avec TOUS les algorithmes RL :
- Algorithmes basÃ©s expÃ©rience : Q-Learning, SARSA, Monte Carlo
- Algorithmes basÃ©s modÃ¨le : Policy Iteration, Value Iteration, Dynamic Programming
"""

import numpy as np
from typing import Tuple, List, Dict, Any

from src.rl_environments.base_environment import BaseEnvironment


class LineWorld(BaseEnvironment):
    """
    Environnement Line World selon les spÃ©cifications du projet.
    
    Compatible avec TOUS les algorithmes d'apprentissage par renforcement :
    - ExpÃ©rience : Q-Learning, SARSA, Monte Carlo (via step/reset)
    - ModÃ¨le : Policy Iteration, Value Iteration (via matrices de transition)
    """
    
    # Actions
    ACTION_LEFT = 0
    ACTION_RIGHT = 1
    ACTION_NAMES = {ACTION_LEFT: "Left", ACTION_RIGHT: "Right"}
    
    REWARD_NEGATIVE = -1.0  # Position 0
    REWARD_NEUTRAL = 0.0    # DÃ©placements normaux
    REWARD_POSITIVE = 1.0   # Position 4
    
    def __init__(self, max_steps: int = 100):
        """
        Initialise LineWorld avec les rÃ¨gles fixes du projet.
        
        Args:
            max_steps: Nombre maximum d'Ã©tapes par Ã©pisode
        """
        super().__init__("LineWorld")
        
        self.max_steps = max_steps
        self.steps_taken = 0
        
        # Configuration fixe selon les spÃ©cifications
        self.states = [0, 1, 2, 3, 4]
        self.actions = [0, 1]  # Left, Right
        self.terminal_states = [0, 4]
        self.start_position = 2 
        
        # Matrices pour les algorithmes basÃ©s modÃ¨le
        self._setup_model_matrices()
        
        # Transitions pour les algorithmes basÃ©s expÃ©rience
        self._setup_transitions()
        
        self.current_state = None
        self.reset()
    
    def _setup_model_matrices(self):
        """Configure les matrices complÃ¨tes pour algorithmes basÃ©s modÃ¨le."""
        num_states = len(self.states)
        num_actions = len(self.actions)
        
        # Matrice de transition P(s'|s,a) : [state][action][next_state]
        self.transition_matrix = np.zeros((num_states, num_actions, num_states))
        
        # Matrice de rÃ©compenses R(s,a,s') : [state][action][next_state]
        self.reward_matrix = np.zeros((num_states, num_actions, num_states))
        
        # Remplissage selon les transitions dÃ©finies
        # Position 1: Left â†’ 0 (reward -1), Right â†’ 2 (reward 0)
        self.transition_matrix[1, 0, 0] = 1.0
        self.reward_matrix[1, 0, 0] = self.REWARD_NEGATIVE
        
        self.transition_matrix[1, 1, 2] = 1.0
        self.reward_matrix[1, 1, 2] = self.REWARD_NEUTRAL
        
        # Position 2: Left â†’ 1 (reward 0), Right â†’ 3 (reward 0)
        self.transition_matrix[2, 0, 1] = 1.0
        self.reward_matrix[2, 0, 1] = self.REWARD_NEUTRAL
        
        self.transition_matrix[2, 1, 3] = 1.0
        self.reward_matrix[2, 1, 3] = self.REWARD_NEUTRAL
        
        # Position 3: Left â†’ 2 (reward 0), Right â†’ 4 (reward 1)
        self.transition_matrix[3, 0, 2] = 1.0
        self.reward_matrix[3, 0, 2] = self.REWARD_NEUTRAL
        
        self.transition_matrix[3, 1, 4] = 1.0
        self.reward_matrix[3, 1, 4] = self.REWARD_POSITIVE
        
        # Ã‰tats terminaux (restent sur place)
        for terminal_state in self.terminal_states:
            for action in self.actions:
                self.transition_matrix[terminal_state, action, terminal_state] = 1.0
                self.reward_matrix[terminal_state, action, terminal_state] = 0.0
    
    def _setup_transitions(self):
        """Configure les transitions pour algorithmes basÃ©s expÃ©rience."""
        # Matrice de transition simple: [Ã©tat][action] = (next_state, reward)
        self.transitions = {
            1: {0: (0, self.REWARD_NEGATIVE), 1: (2, self.REWARD_NEUTRAL)},
            2: {0: (1, self.REWARD_NEUTRAL), 1: (3, self.REWARD_NEUTRAL)},
            3: {0: (2, self.REWARD_NEUTRAL), 1: (4, self.REWARD_POSITIVE)}
        }
    
    @property
    def state_space_size(self) -> int:
        """Retourne la taille de l'espace d'Ã©tats."""
        return len(self.states)
    
    @property
    def action_space_size(self) -> int:
        """Retourne la taille de l'espace d'actions."""
        return len(self.actions)
    
    @property
    def valid_actions(self) -> List[int]:
        """Retourne les actions valides dans l'Ã©tat actuel."""
        if self.current_state in self.terminal_states:
            return []
        return self.actions.copy()
    
    def reset(self) -> int:
        """
        Remet l'environnement Ã  l'Ã©tat initial.
        
        Returns:
            Ã‰tat initial
        """
        self.current_state = self.start_position
        self.steps_taken = 0
        self._reset_episode_stats()
        return self.current_state
    
    def step(self, action: int) -> Tuple[int, float, bool, Dict[str, Any]]:
        """
        ExÃ©cute une action selon les rÃ¨gles du professeur.
        
        Args:
            action: 0 (Left) ou 1 (Right)
            
        Returns:
            (next_state, reward, done, info)
        """
        if action not in self.valid_actions:
            raise ValueError(f"Action invalide: {action}. Actions valides: {self.valid_actions}")
        
        if self.current_state in self.terminal_states:
            raise ValueError(f"Ã‰pisode terminÃ©, impossible d'agir depuis l'Ã©tat {self.current_state}")
        
        self.steps_taken += 1
        old_state = self.current_state
        
        # Transition selon les rÃ¨gles
        if self.current_state in self.transitions:
            next_state, reward = self.transitions[self.current_state][action]
        else:
            next_state, reward = self.current_state, 0.0
        
        self.current_state = next_state
        
        # VÃ©rification fin d'Ã©pisode
        done = (next_state in self.terminal_states) or (self.steps_taken >= self.max_steps)
        
        # Informations supplÃ©mentaires
        info = {
            "action_name": self.ACTION_NAMES[action],
            "old_state": old_state,
            "terminal_reached": next_state in self.terminal_states,
            "max_steps_reached": self.steps_taken >= self.max_steps,
            "steps_taken": self.steps_taken,
            "target_reached": next_state == 4  # Pour compatibilitÃ© avec Agent
        }
        
        # Mise Ã  jour statistiques
        self._update_episode_stats(action, reward, next_state, done)
        
        return next_state, reward, done, info
    
    def render(self, mode: str = 'console'):
        """
        Affiche l'Ã©tat actuel de l'environnement.
        
        Args:
            mode: 'console' ou 'pygame'
        """
        if mode == 'console':
            self._render_console()
        elif mode == 'pygame':
            return self._render_pygame()
        else:
            raise ValueError(f"Mode non supportÃ©: {mode}")
    
    def _render_console(self):
        """Affichage console simple et clair."""
        print(f"\n=== LineWorld (Step {self.steps_taken}) ===")
        
        # Ligne des positions
        position_line = "Pos: "
        visual_line = "     "
        
        for pos in self.states:
            position_line += f"{pos:2d} "
            
            if pos == self.current_state:
                if pos == 0:
                    visual_line += "[A]"  # Agent sur case perdante
                elif pos == 4:
                    visual_line += "[A]"  # Agent sur case gagnante  
                else:
                    visual_line += "[A]"  # Agent en position normale
            elif pos == 0:
                visual_line += "(-)"   # Case perdante
            elif pos == 4:
                visual_line += "(+)"   # Case gagnante
            else:
                visual_line += " . "   # Case normale
        
        print(position_line)
        print(visual_line)
        
        # Ã‰tat et rÃ©compenses
        print(f"Agent en position: {self.current_state}")
        print(f"RÃ©compense totale: {self.total_reward:.1f}")
        
        # Actions disponibles
        if self.valid_actions:
            print("Actions: [0] Left â† | [1] Right â†’")
        else:
            print("Ã‰tat terminal - Aucune action possible")
    
    def _render_pygame(self):
        """Retourne les donnÃ©es pour rendu PyGame."""
        return {
            'positions': self.states,
            'current_position': self.current_state,
            'terminal_states': self.terminal_states,
            'steps_taken': self.steps_taken,
            'total_reward': self.total_reward,
            'valid_actions': self.valid_actions,
            'max_steps': self.max_steps
        }
    
    def get_state_description(self, state: int) -> str:
        """
        Description textuelle d'un Ã©tat.
        
        Args:
            state: Ã‰tat Ã  dÃ©crire
            
        Returns:
            Description de l'Ã©tat
        """
        if state == 0:
            return f"Position {state} (TERMINAL - Perte)"
        elif state == 4:
            return f"Position {state} (TERMINAL - Victoire)"
        else:
            return f"Position {state} (Normal)"
    
    # ============ MÃ‰THODES POUR ALGORITHMES BASÃ‰S MODÃˆLE ============
    
    def get_transition_matrix(self):
        """
        Retourne la matrice de transition complÃ¨te P(s'|s,a).
        
        UtilisÃ©e par : Policy Iteration, Value Iteration, Dynamic Programming
        
        Returns:
            np.ndarray: Matrice [state][action][next_state] = probabilitÃ©
        """
        return self.transition_matrix.copy()
    
    def get_reward_matrix(self):
        """
        Retourne la matrice de rÃ©compenses complÃ¨te R(s,a,s').
        
        UtilisÃ©e par : Policy Iteration, Value Iteration, Dynamic Programming
        
        Returns:
            np.ndarray: Matrice [state][action][next_state] = rÃ©compense
        """
        return self.reward_matrix.copy()
    
    def get_terminal_states(self):
        """
        Retourne les Ã©tats terminaux.
        
        UtilisÃ©e par : Tous les algorithmes basÃ©s modÃ¨le
        
        Returns:
            List[int]: Liste des Ã©tats terminaux
        """
        return self.terminal_states.copy()
    
    def get_all_states(self):
        """Retourne tous les Ã©tats possibles."""
        return self.states.copy()
    
    def get_all_actions(self):
        """Retourne toutes les actions possibles."""
        return self.actions.copy()
    
    # ============ MÃ‰THODES POUR ALGORITHMES BASÃ‰S EXPÃ‰RIENCE ============
    
    def get_optimal_policy(self) -> Dict[int, int]:
        """
        Retourne la politique optimale.
        
        Returns:
            Dict {state: optimal_action}
        """
        # Politique optimale: toujours aller vers la droite (position 4)
        return {
            1: self.ACTION_RIGHT,  # 1 â†’ 2
            2: self.ACTION_RIGHT,  # 2 â†’ 3  
            3: self.ACTION_RIGHT   # 3 â†’ 4 (victoire)
        }
    
    def get_reward_function(self, state: int, action: int, next_state: int) -> float:
        """
        Fonction de rÃ©compense.
        
        Args:
            state: Ã‰tat de dÃ©part
            action: Action exÃ©cutÃ©e  
            next_state: Ã‰tat d'arrivÃ©e
            
        Returns:
            RÃ©compense de la transition
        """
        if state in self.transitions and action in self.transitions[state]:
            _, reward = self.transitions[state][action]
            return reward
        else:
            return 0.0
    
    def is_terminal(self, state: int) -> bool:
        """VÃ©rifie si un Ã©tat est terminal."""
        return state in self.terminal_states


# Fonction utilitaire pour crÃ©er l'environnement standard
def create_lineworld():
    """CrÃ©e l'environnement LineWorld standard."""
    return LineWorld()


if __name__ == "__main__":
    # Test rapide
    print("ğŸ§ª Test LineWorld")
    env = create_lineworld()
    
    print("Ã‰tat initial:")
    env.render()
    
    print("\nTest de quelques actions:")
    actions_test = [1, 1, 0, 1]  # Right, Right, Left, Right
    
    for i, action in enumerate(actions_test):
        if not env.valid_actions:
            print("Ã‰pisode terminÃ©!")
            break
            
        print(f"\n--- Action {i+1}: {env.ACTION_NAMES[action]} ---")
        state, reward, done, info = env.step(action)
        print(f"RÃ©sultat: Ã‰tat {state}, Reward {reward}")
        env.render()
        
        if done:
            print(f"Ã‰pisode terminÃ©! Success: {info['target_reached']}")
            break
    
    # Test des matrices pour algorithmes basÃ©s modÃ¨le
    print("\nğŸ” Test matrices pour Dynamic Programming:")
    print("Matrice de transition shape:", env.get_transition_matrix().shape)
    print("Matrice de rÃ©compenses shape:", env.get_reward_matrix().shape)
    print("Ã‰tats terminaux:", env.get_terminal_states())