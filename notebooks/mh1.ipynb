{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLVaRqFTpnJb"
      },
      "outputs": [],
      "source": [
        "# Comparaison des algorithmes de RL sur Monty Hall 1\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from src.rl_environments.monty_hall_interactive import MontyHallInteractive\n",
        "from src.rl_algorithms.q_learning import QLearning\n",
        "from src.rl_algorithms.sarsa import SARSA\n",
        "from src.rl_algorithms.dyna_q import DynaQ\n",
        "from src.rl_algorithms.monte_carlo_es import MonteCarloES\n",
        "from src.rl_algorithms.off_policy_mc_control import OffPolicyMCControl\n",
        "from src.rl_algorithms.on_policy_first_visit_mc_control import OnPolicyFirstVisitMCControl\n",
        "from src.rl_algorithms.policy_iteration import PolicyIteration\n",
        "from src.rl_algorithms.value_iteration import ValueIteration\n",
        "\n",
        "# Initialisation de l'environnement Monty Hall 1\n",
        "env = MontyHallInteractive()\n",
        "output_dir = 'outputs/mh1'\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_ql = QLearning(\n",
        "    state_space_size=env.state_space_size,\n",
        "    action_space_size=env.action_space_size,\n",
        "    learning_rate=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01\n",
        ")\n",
        "num_episodes = 5000\n",
        "start_time = time.time()\n",
        "train_stats_ql = agent_ql.train(env, num_episodes=num_episodes, verbose=True)\n",
        "train_time_ql = time.time() - start_time\n",
        "\n",
        "agent_ql.save_model(os.path.join(output_dir, 'q_learning_model.pkl'))\n",
        "with open(os.path.join(output_dir, 'q_learning_description.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        \"algorithm\": \"Q-Learning\",\n",
        "        \"num_episodes\": num_episodes,\n",
        "        \"train_time_seconds\": train_time_ql,\n",
        "        \"final_avg_reward\": train_stats_ql['final_avg_reward'],\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": agent_ql.learning_rate,\n",
        "            \"gamma\": agent_ql.gamma,\n",
        "            \"epsilon\": agent_ql.epsilon,\n",
        "            \"epsilon_decay\": agent_ql.epsilon_decay,\n",
        "            \"epsilon_min\": agent_ql.epsilon_min\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "qlearning_metrics = {\n",
        "    \"rewards\": train_stats_ql['episode_rewards'],\n",
        "    \"lengths\": train_stats_ql['episode_lengths'],\n",
        "    \"train_time\": train_time_ql\n",
        "}\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(qlearning_metrics['rewards'])\n",
        "plt.title('Récompense par épisode (Q-Learning)')\n",
        "plt.xlabel('Épisode')\n",
        "plt.ylabel('Récompense')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(\"Politique apprise (état -> action):\", agent_ql.get_policy())\n",
        "print(\"Q-table finale :\", agent_ql.q_function)"
      ],
      "metadata": {
        "id": "8JLB31n3pudp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_sarsa = SARSA(\n",
        "    state_space_size=env.state_space_size,\n",
        "    action_space_size=env.action_space_size,\n",
        "    learning_rate=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01\n",
        ")\n",
        "num_episodes_sarsa = 5000\n",
        "start_time_sarsa = time.time()\n",
        "train_stats_sarsa = agent_sarsa.train(env, num_episodes=num_episodes_sarsa, verbose=True)\n",
        "train_time_sarsa = time.time() - start_time_sarsa\n",
        "\n",
        "agent_sarsa.save_model(os.path.join(output_dir, 'sarsa_model.pkl'))\n",
        "with open(os.path.join(output_dir, 'sarsa_description.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        \"algorithm\": \"SARSA\",\n",
        "        \"num_episodes\": num_episodes_sarsa,\n",
        "        \"train_time_seconds\": train_time_sarsa,\n",
        "        \"final_avg_reward\": train_stats_sarsa['final_avg_reward'],\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": agent_sarsa.learning_rate,\n",
        "            \"gamma\": agent_sarsa.gamma,\n",
        "            \"epsilon\": agent_sarsa.epsilon,\n",
        "            \"epsilon_decay\": agent_sarsa.epsilon_decay,\n",
        "            \"epsilon_min\": agent_sarsa.epsilon_min\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "sarsa_metrics = {\n",
        "    \"rewards\": train_stats_sarsa['episode_rewards'],\n",
        "    \"lengths\": train_stats_sarsa['episode_lengths'],\n",
        "    \"train_time\": train_time_sarsa\n",
        "}\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(sarsa_metrics['rewards'])\n",
        "plt.title('Récompense par épisode (SARSA)')\n",
        "plt.xlabel('Épisode')\n",
        "plt.ylabel('Récompense')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(\"Politique apprise (état -> action):\", agent_sarsa.get_policy())\n",
        "print(\"Q-table finale :\", agent_sarsa.q_function)"
      ],
      "metadata": {
        "id": "dCzfD1lGpzCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_dynaq = DynaQ(\n",
        "    state_space_size=env.state_space_size,\n",
        "    action_space_size=env.action_space_size,\n",
        "    learning_rate=0.1,\n",
        "    gamma=0.99,\n",
        "    epsilon=0.1,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01,\n",
        "    n_planning_steps=10\n",
        ")\n",
        "num_episodes_dynaq = 5000\n",
        "start_time_dynaq = time.time()\n",
        "train_stats_dynaq = agent_dynaq.train(env, num_episodes=num_episodes_dynaq, verbose=True)\n",
        "train_time_dynaq = time.time() - start_time_dynaq\n",
        "\n",
        "agent_dynaq.save_model(os.path.join(output_dir, 'dynaq_model.pkl'))\n",
        "with open(os.path.join(output_dir, 'dynaq_description.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        \"algorithm\": \"Dyna-Q\",\n",
        "        \"num_episodes\": num_episodes_dynaq,\n",
        "        \"train_time_seconds\": train_time_dynaq,\n",
        "        \"final_avg_reward\": train_stats_dynaq['final_avg_reward'],\n",
        "        \"hyperparameters\": {\n",
        "            \"learning_rate\": agent_dynaq.learning_rate,\n",
        "            \"gamma\": agent_dynaq.gamma,\n",
        "            \"epsilon\": agent_dynaq.epsilon,\n",
        "            \"epsilon_decay\": agent_dynaq.epsilon_decay,\n",
        "            \"epsilon_min\": agent_dynaq.epsilon_min,\n",
        "            \"n_planning_steps\": agent_dynaq.n_planning_steps\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "dynaq_metrics = {\n",
        "    \"rewards\": train_stats_dynaq['episode_rewards'],\n",
        "    \"lengths\": train_stats_dynaq['episode_lengths'],\n",
        "    \"train_time\": train_time_dynaq\n",
        "}\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(dynaq_metrics['rewards'])\n",
        "plt.title('Récompense par épisode (Dyna-Q)')\n",
        "plt.xlabel('Épisode')\n",
        "plt.ylabel('Récompense')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(\"Politique apprise (état -> action):\", agent_dynaq.get_policy())\n",
        "print(\"Q-table finale :\", agent_dynaq.q_function)"
      ],
      "metadata": {
        "id": "-wnDMvBCp1oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_mc_es = MonteCarloES(\n",
        "    state_space_size=env.state_space_size,\n",
        "    action_space_size=env.action_space_size,\n",
        "    gamma=0.99,\n",
        "    epsilon=0.1,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01\n",
        ")\n",
        "num_episodes_mc_es = 5000\n",
        "start_time_mc_es = time.time()\n",
        "train_stats_mc_es = agent_mc_es.train(env, num_episodes=num_episodes_mc_es, verbose=True)\n",
        "train_time_mc_es = time.time() - start_time_mc_es\n",
        "\n",
        "agent_mc_es.save_model(os.path.join(output_dir, 'mc_es_model.pkl'))\n",
        "with open(os.path.join(output_dir, 'mc_es_description.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        \"algorithm\": \"Monte Carlo ES\",\n",
        "        \"num_episodes\": num_episodes_mc_es,\n",
        "        \"train_time_seconds\": train_time_mc_es,\n",
        "        \"final_avg_reward\": train_stats_mc_es['final_avg_reward'],\n",
        "        \"hyperparameters\": {\n",
        "            \"gamma\": agent_mc_es.gamma,\n",
        "            \"epsilon\": agent_mc_es.epsilon,\n",
        "            \"epsilon_decay\": agent_mc_es.epsilon_decay,\n",
        "            \"epsilon_min\": agent_mc_es.epsilon_min\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "mc_es_metrics = {\n",
        "    \"rewards\": train_stats_mc_es['episode_rewards'],\n",
        "    \"lengths\": train_stats_mc_es['episode_lengths'],\n",
        "    \"train_time\": train_time_mc_es\n",
        "}\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(mc_es_metrics['rewards'])\n",
        "plt.title('Récompense par épisode (Monte Carlo ES)')\n",
        "plt.xlabel('Épisode')\n",
        "plt.ylabel('Récompense')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(\"Politique apprise (état -> action):\", agent_mc_es.get_policy())\n",
        "print(\"Q-table finale :\", agent_mc_es.q_function)"
      ],
      "metadata": {
        "id": "aIGzcgsrp5QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_offpolicy_mc = OffPolicyMCControl(\n",
        "    state_space_size=env.state_space_size,\n",
        "    action_space_size=env.action_space_size,\n",
        "    gamma=0.99,\n",
        "    epsilon=0.1,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01\n",
        ")\n",
        "num_episodes_offpolicy_mc = 5000\n",
        "start_time_offpolicy_mc = time.time()\n",
        "train_stats_offpolicy_mc = agent_offpolicy_mc.train(env, num_episodes=num_episodes_offpolicy_mc, verbose=True)\n",
        "train_time_offpolicy_mc = time.time() - start_time_offpolicy_mc\n",
        "\n",
        "agent_offpolicy_mc.save_model(os.path.join(output_dir, 'offpolicy_mc_model.pkl'))\n",
        "with open(os.path.join(output_dir, 'offpolicy_mc_description.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        \"algorithm\": \"Off-policy MC Control\",\n",
        "        \"num_episodes\": num_episodes_offpolicy_mc,\n",
        "        \"train_time_seconds\": train_time_offpolicy_mc,\n",
        "        \"final_avg_reward\": train_stats_offpolicy_mc['final_avg_reward'],\n",
        "        \"hyperparameters\": {\n",
        "            \"gamma\": agent_offpolicy_mc.gamma,\n",
        "            \"epsilon\": agent_offpolicy_mc.epsilon,\n",
        "            \"epsilon_decay\": agent_offpolicy_mc.epsilon_decay,\n",
        "            \"epsilon_min\": agent_offpolicy_mc.epsilon_min\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "offpolicy_mc_metrics = {\n",
        "    \"rewards\": train_stats_offpolicy_mc['episode_rewards'],\n",
        "    \"lengths\": train_stats_offpolicy_mc['episode_lengths'],\n",
        "    \"train_time\": train_time_offpolicy_mc\n",
        "}\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(offpolicy_mc_metrics['rewards'])\n",
        "plt.title('Récompense par épisode (Off-policy MC Control)')\n",
        "plt.xlabel('Épisode')\n",
        "plt.ylabel('Récompense')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(\"Politique apprise (état -> action):\", agent_offpolicy_mc.get_policy())\n",
        "print(\"Q-table finale :\", agent_offpolicy_mc.q_function)"
      ],
      "metadata": {
        "id": "4A59BrPvp7qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_onpolicy_mc = OnPolicyFirstVisitMCControl(\n",
        "    state_space_size=env.state_space_size,\n",
        "    action_space_size=env.action_space_size,\n",
        "    gamma=0.99,\n",
        "    epsilon=0.1,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01\n",
        ")\n",
        "num_episodes_onpolicy_mc = 5000\n",
        "start_time_onpolicy_mc = time.time()\n",
        "train_stats_onpolicy_mc = agent_onpolicy_mc.train(env, num_episodes=num_episodes_onpolicy_mc, verbose=True)\n",
        "train_time_onpolicy_mc = time.time() - start_time_onpolicy_mc\n",
        "\n",
        "agent_onpolicy_mc.save_model(os.path.join(output_dir, 'onpolicy_mc_model.pkl'))\n",
        "with open(os.path.join(output_dir, 'onpolicy_mc_description.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        \"algorithm\": \"On-policy First Visit MC Control\",\n",
        "        \"num_episodes\": num_episodes_onpolicy_mc,\n",
        "        \"train_time_seconds\": train_time_onpolicy_mc,\n",
        "        \"final_avg_reward\": train_stats_onpolicy_mc['final_avg_reward'],\n",
        "        \"hyperparameters\": {\n",
        "            \"gamma\": agent_onpolicy_mc.gamma,\n",
        "            \"epsilon\": agent_onpolicy_mc.epsilon,\n",
        "            \"epsilon_decay\": agent_onpolicy_mc.epsilon_decay,\n",
        "            \"epsilon_min\": agent_onpolicy_mc.epsilon_min\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "onpolicy_mc_metrics = {\n",
        "    \"rewards\": train_stats_onpolicy_mc['episode_rewards'],\n",
        "    \"lengths\": train_stats_onpolicy_mc['episode_lengths'],\n",
        "    \"train_time\": train_time_onpolicy_mc\n",
        "}\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(onpolicy_mc_metrics['rewards'])\n",
        "plt.title('Récompense par épisode (On-policy First Visit MC Control)')\n",
        "plt.xlabel('Épisode')\n",
        "plt.ylabel('Récompense')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(\"Politique apprise (état -> action):\", agent_onpolicy_mc.get_policy())\n",
        "print(\"Q-table finale :\", agent_onpolicy_mc.q_function)"
      ],
      "metadata": {
        "id": "mJBT0BeZqBNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_policy_iter = PolicyIteration(\n",
        "    state_space_size=env.state_space_size,\n",
        "    action_space_size=env.action_space_size,\n",
        "    gamma=0.999999,\n",
        "    theta=0.00001,\n",
        "    max_iterations=1000\n",
        ")\n",
        "start_time_policy_iter = time.time()\n",
        "train_stats_policy_iter = agent_policy_iter.train(env, verbose=True)\n",
        "train_time_policy_iter = time.time() - start_time_policy_iter\n",
        "\n",
        "agent_policy_iter.save_model(os.path.join(output_dir, 'policy_iteration_model.pkl'))\n",
        "with open(os.path.join(output_dir, 'policy_iteration_description.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        \"algorithm\": \"Policy Iteration\",\n",
        "        \"iterations\": train_stats_policy_iter['iterations'],\n",
        "        \"converged\": train_stats_policy_iter['converged'],\n",
        "        \"max_value\": train_stats_policy_iter['max_value'],\n",
        "        \"train_time_seconds\": train_time_policy_iter,\n",
        "        \"hyperparameters\": {\n",
        "            \"gamma\": agent_policy_iter.gamma,\n",
        "            \"theta\": agent_policy_iter.theta,\n",
        "            \"max_iterations\": agent_policy_iter.max_iterations\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "policy_iter_metrics = {\n",
        "    \"iterations\": train_stats_policy_iter['iterations'],\n",
        "    \"train_time\": train_time_policy_iter\n",
        "}\n",
        "print(\"Politique apprise (état -> action):\", agent_policy_iter.get_policy())\n",
        "print(\"Fonction de valeur finale :\", agent_policy_iter.get_value_function())"
      ],
      "metadata": {
        "id": "QKGrEpu9qBIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_value_iter = ValueIteration(\n",
        "    state_space_size=env.state_space_size,\n",
        "    action_space_size=env.action_space_size,\n",
        "    gamma=0.9,\n",
        "    theta=1e-6,\n",
        "    max_iterations=1000\n",
        ")\n",
        "start_time_value_iter = time.time()\n",
        "train_stats_value_iter = agent_value_iter.train(env, verbose=True)\n",
        "train_time_value_iter = time.time() - start_time_value_iter\n",
        "\n",
        "agent_value_iter.save_model(os.path.join(output_dir, 'value_iteration_model.pkl'))\n",
        "with open(os.path.join(output_dir, 'value_iteration_description.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        \"algorithm\": \"Value Iteration\",\n",
        "        \"iterations\": train_stats_value_iter['iterations'],\n",
        "        \"converged\": train_stats_value_iter['converged'],\n",
        "        \"max_value\": train_stats_value_iter['max_value'],\n",
        "        \"train_time_seconds\": train_time_value_iter,\n",
        "        \"hyperparameters\": {\n",
        "            \"gamma\": agent_value_iter.gamma,\n",
        "            \"theta\": agent_value_iter.theta,\n",
        "            \"max_iterations\": agent_value_iter.max_iterations\n",
        "        }\n",
        "    }, f, indent=2)\n",
        "value_iter_metrics = {\n",
        "    \"iterations\": train_stats_value_iter['iterations'],\n",
        "    \"train_time\": train_time_value_iter\n",
        "}\n",
        "print(\"Politique apprise (état -> action):\", agent_value_iter.get_policy())\n",
        "print(\"Fonction de valeur finale :\", agent_value_iter.get_value_function())"
      ],
      "metadata": {
        "id": "kTk0THzkqBCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Courbes de récompenses (pour les algos par expérience)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(qlearning_metrics['rewards'], label='Q-Learning')\n",
        "plt.plot(sarsa_metrics['rewards'], label='SARSA')\n",
        "plt.plot(dynaq_metrics['rewards'], label='Dyna-Q')\n",
        "plt.plot(mc_es_metrics['rewards'], label='Monte Carlo ES')\n",
        "plt.plot(offpolicy_mc_metrics['rewards'], label='Off-policy MC Control')\n",
        "plt.plot(onpolicy_mc_metrics['rewards'], label='On-policy First Visit MC Control')\n",
        "plt.title('Courbe de récompense par épisode')\n",
        "plt.xlabel('Épisode')\n",
        "plt.ylabel('Récompense')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Tableau récapitulatif\n",
        "summary_data = {\n",
        "    'Algorithme': [\n",
        "        'Q-Learning', 'SARSA', 'Dyna-Q', 'Monte Carlo ES',\n",
        "        'Off-policy MC Control', 'On-policy First Visit MC Control',\n",
        "        'Policy Iteration', 'Value Iteration'\n",
        "    ],\n",
        "    'Temps entraînement (s)': [\n",
        "        qlearning_metrics['train_time'],\n",
        "        sarsa_metrics['train_time'],\n",
        "        dynaq_metrics['train_time'],\n",
        "        mc_es_metrics['train_time'],\n",
        "        offpolicy_mc_metrics['train_time'],\n",
        "        onpolicy_mc_metrics['train_time'],\n",
        "        policy_iter_metrics['train_time'],\n",
        "        value_iter_metrics['train_time']\n",
        "    ],\n",
        "    'Convergence/Itérations': [\n",
        "        len(qlearning_metrics['rewards']),\n",
        "        len(sarsa_metrics['rewards']),\n",
        "        len(dynaq_metrics['rewards']),\n",
        "        len(mc_es_metrics['rewards']),\n",
        "        len(offpolicy_mc_metrics['rewards']),\n",
        "        len(onpolicy_mc_metrics['rewards']),\n",
        "        policy_iter_metrics['iterations'],\n",
        "        value_iter_metrics['iterations']\n",
        "    ]\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "display(summary_df)\n",
        "\n",
        "# Politiques optimales\n",
        "print('Politiques optimales apprises :')\n",
        "print('Q-Learning:', agent_ql.get_policy())\n",
        "print('SARSA:', agent_sarsa.get_policy())\n",
        "print('Dyna-Q:', agent_dynaq.get_policy())\n",
        "print('Monte Carlo ES:', agent_mc_es.get_policy())\n",
        "print('Off-policy MC Control:', agent_offpolicy_mc.get_policy())\n",
        "print('On-policy First Visit MC Control:', agent_onpolicy_mc.get_policy())\n",
        "print('Policy Iteration:', agent_policy_iter.get_policy())\n",
        "print('Value Iteration:', agent_value_iter.get_policy())"
      ],
      "metadata": {
        "id": "ZT8NvyyJqA_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djaELzaLqA9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yotdk5-ZqA6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w37E4agFqAxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HCLmMuf5qAt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ekqS-YmqAq8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}