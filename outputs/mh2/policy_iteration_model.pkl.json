{
  "algorithm": "PolicyIteration",
  "state_space_size": 5,
  "action_space_size": 5,
  "gamma": 0.999999,
  "theta": 1e-05,
  "is_trained": true,
  "value_function": [
    0.34273185971156495,
    0.9374883740539502,
    0.9436393025982766,
    0.09704007157230028,
    0.0
  ],
  "policy": [
    [
      1.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    [
      1.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    [
      1.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    [
      1.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    [
      0.2,
      0.2,
      0.2,
      0.2,
      0.2
    ]
  ],
  "q_function": [
    [
      0.3427315169797052,
      0.3427315169797052,
      0.3427315169797052,
      0.3427315169797052,
      0.3427315169797052
    ],
    [
      0.9374874365655762,
      0.9374874365655762,
      0.9374874365655762,
      0.9374874365655762,
      0.9374874365655762
    ],
    [
      0.943638358958974,
      0.943638358958974,
      0.943638358958974,
      0.943638358958974,
      0.943638358958974
    ],
    [
      0.0970399745322287,
      0.0970399745322287,
      0.0970399745322287,
      0.0970399745322287,
      0.0970399745322287
    ],
    [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ]
  ],
  "convergence_history": [
    {
      "iteration": 0,
      "policy_stable": true,
      "max_value": 0.9436393025982766
    }
  ],
  "training_history": [
    {
      "episode": 1,
      "reward": 0.4641799215872184,
      "steps": 1
    }
  ]
}