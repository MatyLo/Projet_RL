{
  "algorithm": "ValueIteration",
  "state_space_size": 11,
  "action_space_size": 3,
  "gamma": 0.9,
  "theta": 1e-06,
  "is_trained": true,
  "value_function": [
    0.9,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    0.0
  ],
  "q_function": [
    [
      0.8999999999999999,
      0.9,
      0.8999999999999999
    ],
    [
      0.0,
      1.0,
      -1.0
    ],
    [
      0.0,
      1.0,
      -1.0
    ],
    [
      0.0,
      1.0,
      -1.0
    ],
    [
      -1.0,
      0.0,
      1.0
    ],
    [
      -1.0,
      0.0,
      1.0
    ],
    [
      -1.0,
      0.0,
      1.0
    ],
    [
      1.0,
      -1.0,
      0.0
    ],
    [
      1.0,
      -1.0,
      0.0
    ],
    [
      1.0,
      -1.0,
      0.0
    ],
    [
      0.0,
      0.0,
      0.0
    ]
  ],
  "policy": [
    1,
    1,
    1,
    1,
    2,
    2,
    2,
    0,
    0,
    0,
    0
  ],
  "convergence_history": [
    {
      "iteration": 0,
      "delta": 1.0,
      "max_value": 1.0
    },
    {
      "iteration": 1,
      "delta": 0.9,
      "max_value": 1.0
    },
    {
      "iteration": 2,
      "delta": 0,
      "max_value": 1.0
    }
  ],
  "training_history": [
    {
      "episode": 1,
      "reward": 0.9,
      "steps": 3
    }
  ]
}