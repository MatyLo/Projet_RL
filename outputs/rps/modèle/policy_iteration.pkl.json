{
  "algorithm": "PolicyIteration",
  "state_space_size": 11,
  "action_space_size": 3,
  "gamma": 0.999999,
  "theta": 1e-05,
  "is_trained": true,
  "value_function": [
    0.9999989999999999,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    0.0
  ],
  "policy": [
    [
      1.0,
      0.0,
      0.0
    ],
    [
      0.0,
      1.0,
      0.0
    ],
    [
      0.0,
      1.0,
      0.0
    ],
    [
      0.0,
      1.0,
      0.0
    ],
    [
      0.0,
      0.0,
      1.0
    ],
    [
      0.0,
      0.0,
      1.0
    ],
    [
      0.0,
      0.0,
      1.0
    ],
    [
      1.0,
      0.0,
      0.0
    ],
    [
      1.0,
      0.0,
      0.0
    ],
    [
      1.0,
      0.0,
      0.0
    ],
    [
      0.3333333333333333,
      0.3333333333333333,
      0.3333333333333333
    ]
  ],
  "q_function": [
    [
      0.9999989999999999,
      0.9999989999999999,
      0.9999989999999999
    ],
    [
      0.0,
      1.0,
      -1.0
    ],
    [
      0.0,
      1.0,
      -1.0
    ],
    [
      0.0,
      1.0,
      -1.0
    ],
    [
      -1.0,
      0.0,
      1.0
    ],
    [
      -1.0,
      0.0,
      1.0
    ],
    [
      -1.0,
      0.0,
      1.0
    ],
    [
      1.0,
      -1.0,
      0.0
    ],
    [
      1.0,
      -1.0,
      0.0
    ],
    [
      1.0,
      -1.0,
      0.0
    ],
    [
      0.0,
      0.0,
      0.0
    ]
  ],
  "convergence_history": [
    {
      "iteration": 0,
      "policy_stable": false,
      "max_value": 0.0
    },
    {
      "iteration": 1,
      "policy_stable": true,
      "max_value": 1.0
    }
  ],
  "training_history": [
    {
      "episode": 1,
      "reward": 0.9090908181818181,
      "steps": 2
    }
  ]
}