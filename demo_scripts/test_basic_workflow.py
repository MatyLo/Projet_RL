#!/usr/bin/env python3
"""
Script de test du workflow de base - Line World + Q-Learning

Ce script valide que le workflow complet fonctionne :
1. CrÃ©ation d'un environnement Line World
2. CrÃ©ation d'un algorithme Q-Learning  
3. CrÃ©ation d'un Agent
4. EntraÃ®nement
5. Ã‰valuation
6. DÃ©monstration
7. Sauvegarde/Chargement

Usage:
    python demo_scripts/test_basic_workflow.py

Placement: demo_scripts/test_basic_workflow.py
"""

import sys
import os
import numpy as np

# Ajout des chemins pour imports simples
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)

# Imports simples
try:
    from src.rl_environments.line_world import LineWorld, create_simple_lineworld
    from src.rl_algorithms.temporal_difference.q_learning import QLearning, create_standard_qlearning
    from core.agent import Agent, quick_train_and_evaluate
except ImportError as e:
    print(f"âŒ Erreur d'import: {e}")
    print("ğŸ’¡ VÃ©rifiez que vous lancez depuis la racine du projet:")
    print("   python demo_scripts/test_basic_workflow.py")
    sys.exit(1)


def test_environment_creation():
    """Test de crÃ©ation et fonctionnement de base de l'environnement."""
    print("\n" + "="*60)
    print("TEST 1: CRÃ‰ATION DE L'ENVIRONNEMENT LINE WORLD")
    print("="*60)
    
    try:
        # CrÃ©ation d'un environnement simple
        env = create_simple_lineworld()
        print(f"âœ… Environnement crÃ©Ã©: {env}")
        print(f"   - Espace d'Ã©tats: {env.state_space_size}")
        print(f"   - Espace d'actions: {env.action_space_size}")
        print(f"   - Position cible: {env.target_position}")
        
        # Test de reset
        initial_state = env.reset()
        print(f"   - Ã‰tat initial: {initial_state}")
        
        # Test de quelques actions
        print("\n   Test de 3 actions alÃ©atoires:")
        for i in range(3):
            action = np.random.choice([0, 1])
            action_name = env.ACTION_NAMES[action]
            state, reward, done, info = env.step(action)
            print(f"     Action {action} ({action_name}): Ã©tat={state}, rÃ©compense={reward:.2f}, terminÃ©={done}")
            
            if done:
                print(f"     Episode terminÃ© en {i+1} Ã©tapes!")
                break
        
        # Test de rendu
        print(f"\n   Ã‰tat final de l'environnement:")
        env.render('console')
        
        return True, env
        
    except Exception as e:
        print(f"âŒ Erreur lors du test de l'environnement: {e}")
        return False, None


def test_algorithm_creation():
    """Test de crÃ©ation et fonctionnement de base de l'algorithme."""
    print("\n" + "="*60)
    print("TEST 2: CRÃ‰ATION DE L'ALGORITHME Q-LEARNING")
    print("="*60)
    
    try:
        # CrÃ©ation d'un algorithme Q-Learning
        algorithm = create_standard_qlearning(state_space_size=5, action_space_size=2)
        print(f"âœ… Algorithme crÃ©Ã©: {algorithm}")
        print(f"   - Type: {algorithm.algorithm_type}")
        print(f"   - HyperparamÃ¨tres: {algorithm.get_hyperparameters()}")
        
        # Test de sÃ©lection d'action avant entraÃ®nement
        print(f"\n   Test de sÃ©lection d'actions avant entraÃ®nement:")
        for state in range(3):
            action = algorithm.select_action(state, training=True)
            print(f"     Ã‰tat {state} -> Action {action}")
        
        # Affichage Q-table initiale
        print(f"\n   Q-table initiale:")
        print(algorithm.visualize_q_table())
        
        return True, algorithm
        
    except Exception as e:
        print(f"âŒ Erreur lors du test de l'algorithme: {e}")
        return False, None


def test_agent_creation_and_training(env, algorithm):
    """Test de crÃ©ation d'agent et d'entraÃ®nement."""
    print("\n" + "="*60)
    print("TEST 3: CRÃ‰ATION D'AGENT ET ENTRAÃNEMENT")
    print("="*60)
    
    try:
        # CrÃ©ation de l'agent
        agent = Agent(algorithm, env, "TestAgent_LineWorld_QLearning")
        print(f"âœ… Agent crÃ©Ã©: {agent}")
        
        # EntraÃ®nement rapide
        print(f"\n   DÃ©marrage de l'entraÃ®nement (200 Ã©pisodes)...")
        train_results = agent.train(
            num_episodes=200,
            verbose=True
        )
        
        print(f"\nâœ… EntraÃ®nement terminÃ©!")
        print(f"   - RÃ©compense moyenne: {train_results['avg_reward']:.2f}")
        print(f"   - RÃ©compense finale: {train_results['final_episode_reward']:.2f}")
        print(f"   - Temps d'entraÃ®nement: {train_results['training_time']:.2f}s")
        
        # Affichage Q-table finale
        print(f"\n   Q-table aprÃ¨s entraÃ®nement:")
        print(algorithm.visualize_q_table())
        
        return True, agent
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'entraÃ®nement: {e}")
        return False, None


def test_evaluation_and_demonstration(agent):
    """Test d'Ã©valuation et de dÃ©monstration."""
    print("\n" + "="*60)
    print("TEST 4: Ã‰VALUATION ET DÃ‰MONSTRATION")
    print("="*60)
    
    try:
        # Ã‰valuation de l'agent
        print(f"   Ã‰valuation de l'agent sur 50 Ã©pisodes...")
        eval_results = agent.evaluate(num_episodes=50, verbose=True)
        
        print(f"\nâœ… Ã‰valuation terminÃ©e!")
        print(f"   - Performance moyenne: {eval_results['avg_reward']:.2f} Â± {eval_results['std_reward']:.2f}")
        print(f"   - Meilleure performance: {eval_results['max_reward']:.2f}")
        print(f"   - Longueur moyenne d'Ã©pisode: {eval_results['avg_episode_length']:.1f}")
        
        # DÃ©monstration d'un Ã©pisode
        print(f"\n   DÃ©monstration d'un Ã©pisode:")
        demo_history = agent.demonstrate(
            num_episodes=1,
            render_mode='console',
            step_by_step=False,
            delay_between_steps=0.5
        )
        
        print(f"\nâœ… DÃ©monstration terminÃ©e!")
        episode = demo_history[0]
        print(f"   - RÃ©compense de l'Ã©pisode: {episode['total_reward']:.2f}")
        print(f"   - Nombre d'Ã©tapes: {episode['steps']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'Ã©valuation/dÃ©monstration: {e}")
        return False


def test_save_and_load(agent):
    """Test de sauvegarde et chargement."""
    print("\n" + "="*60)
    print("TEST 5: SAUVEGARDE ET CHARGEMENT")
    print("="*60)
    
    try:
        # Sauvegarde
        save_path = "outputs/test_agent"
        os.makedirs("outputs", exist_ok=True)
        
        print(f"   Sauvegarde de l'agent...")
        save_success = agent.save_agent(save_path)
        
        if save_success:
            print(f"âœ… Agent sauvegardÃ© dans {save_path}")
        else:
            print(f"âŒ Ã‰chec de la sauvegarde")
            return False
        
        # Test de chargement
        print(f"   Test de chargement...")
        
        # CrÃ©er un nouvel agent identique
        new_env = create_simple_lineworld()
        new_algorithm = create_standard_qlearning(
            state_space_size=new_env.state_space_size,
            action_space_size=new_env.action_space_size
        )
        new_agent = Agent(new_algorithm, new_env, "LoadedTestAgent")
        
        load_success = new_agent.load_agent(save_path)
        
        if load_success:
            print(f"âœ… Agent chargÃ© avec succÃ¨s")
            
            # VÃ©rification que l'agent chargÃ© fonctionne
            quick_eval = new_agent.evaluate(num_episodes=10, verbose=False)
            print(f"   - Test de l'agent chargÃ©: rÃ©compense moyenne = {quick_eval['avg_reward']:.2f}")
            
        else:
            print(f"âŒ Ã‰chec du chargement")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde/chargement: {e}")
        return False


def test_policy_analysis(agent):
    """Test d'analyse de la politique apprise."""
    print("\n" + "="*60)
    print("TEST 6: ANALYSE DE LA POLITIQUE APPRISE")
    print("="*60)
    
    try:
        # Affichage de la politique
        print(agent.get_policy_visualization())
        
        # Comparaison avec la politique optimale
        optimal_policy = agent.environment.get_optimal_policy()
        print(f"\n   Politique optimale de rÃ©fÃ©rence:")
        for state, action in optimal_policy.items():
            print(f"     Ã‰tat {state}: Action {action}")
        
        # Comparaison
        learned_policy = agent.algorithm.get_policy()
        correct_actions = 0
        total_states = len(optimal_policy)
        
        print(f"\n   Comparaison avec la politique optimale:")
        for state in range(total_states):
            learned_action = learned_policy[state]
            optimal_action = optimal_policy[state]
            is_correct = learned_action == optimal_action
            
            if is_correct:
                correct_actions += 1
            
            status = "âœ…" if is_correct else "âŒ"
            print(f"     Ã‰tat {state}: Appris={learned_action}, Optimal={optimal_action} {status}")
        
        accuracy = correct_actions / total_states * 100
        print(f"\n   PrÃ©cision de la politique: {accuracy:.1f}% ({correct_actions}/{total_states})")
        
        if accuracy >= 80:
            print(f"âœ… Politique correcte apprise!")
        else:
            print(f"âš ï¸  Politique partiellement correcte - pourrait nÃ©cessiter plus d'entraÃ®nement")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'analyse de politique: {e}")
        return False


def main():
    """Fonction principale de test."""
    print("ğŸš€ DÃ‰BUT DES TESTS DU WORKFLOW DE BASE")
    print("ğŸ¯ Objectif: Valider Line World + Q-Learning + Agent")
    
    tests_passed = 0
    total_tests = 6
    
    # Test 1: Environnement
    success, env = test_environment_creation()
    if success:
        tests_passed += 1
    else:
        print("âŒ Test d'environnement Ã©chouÃ© - arrÃªt des tests")
        return
    
    # Test 2: Algorithme
    success, algorithm = test_algorithm_creation()
    if success:
        tests_passed += 1
    else:
        print("âŒ Test d'algorithme Ã©chouÃ© - arrÃªt des tests")
        return
    
    # Test 3: Agent et entraÃ®nement
    success, agent = test_agent_creation_and_training(env, algorithm)
    if success:
        tests_passed += 1
    else:
        print("âŒ Test d'agent Ã©chouÃ© - arrÃªt des tests")
        return
    
    # Test 4: Ã‰valuation et dÃ©monstration
    success = test_evaluation_and_demonstration(agent)
    if success:
        tests_passed += 1
    
    # Test 5: Sauvegarde et chargement
    success = test_save_and_load(agent)
    if success:
        tests_passed += 1
    
    # Test 6: Analyse de politique
    success = test_policy_analysis(agent)
    if success:
        tests_passed += 1
    
    # RÃ©sumÃ© final
    print("\n" + "="*60)
    print("RÃ‰SUMÃ‰ DES TESTS")
    print("="*60)
    print(f"Tests rÃ©ussis: {tests_passed}/{total_tests}")
    
    if tests_passed == total_tests:
        print("ğŸ‰ TOUS LES TESTS SONT PASSÃ‰S!")
        print("âœ… Le workflow de base fonctionne correctement")
        print("ğŸš€ Vous pouvez maintenant ajouter d'autres algorithmes et environnements")
    else:
        print(f"âš ï¸  {total_tests - tests_passed} test(s) ont Ã©chouÃ©")
        print("ğŸ”§ VÃ©rifiez les erreurs ci-dessus avant de continuer")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    # Configuration de NumPy pour des rÃ©sultats reproductibles
    np.random.seed(42)
    
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸  Tests interrompus par l'utilisateur")
    except Exception as e:
        print(f"\n\nğŸ’¥ Erreur inattendue: {e}")
        import traceback
        traceback.print_exc()