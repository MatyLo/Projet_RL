"""
Test Complet du Workflow Base - Version 4

Ce script teste l'architecture complÃ¨te du projet RL :
1. Chargement configuration JSON
2. CrÃ©ation environnement LineWorld
3. EntraÃ®nement Q-Learning autonome  
4. Agent wrapper post-entraÃ®nement
5. Ã‰valuation et dÃ©monstration
6. Sauvegarde des rÃ©sultats

Usage:
    python demo_scripts/test_base_v4.py
    python demo_scripts/test_base_v4.py --config test_config.json
"""

import sys
import os
import json
import time
import argparse
import pickle
from pathlib import Path

# Ajout des chemins pour imports
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(os.path.join(project_root, 'src'))
sys.path.append(os.path.join(project_root, 'utils'))

try:
    from src.rl_environments.line_world import LineWorld, create_lineworld
    from src.rl_algorithms.temporal_difference.q_learning import QLearning
    from utils.agent import Agent
except ImportError as e:
    print(f"âŒ Erreur d'import: {e}")
    print("ğŸ’¡ VÃ©rifiez que vous lancez depuis la racine du projet")
    sys.exit(1)


def load_config(config_path: str) -> dict:
    """
    Charge la configuration depuis un fichier JSON.
    
    Args:
        config_path: Chemin vers le fichier de configuration
        
    Returns:
        Dict avec la configuration chargÃ©e
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"âœ… Configuration chargÃ©e: {config_path}")
        return config
    except FileNotFoundError:
        print(f"âŒ Fichier de configuration non trouvÃ©: {config_path}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ Erreur JSON dans {config_path}: {e}")
        sys.exit(1)


def create_environment(env_config: dict):
    """
    CrÃ©e l'environnement selon la configuration.
    
    Args:
        env_config: Configuration de l'environnement
        
    Returns:
        Instance de l'environnement
    """
    env_type = env_config.get('type', 'lineworld').lower()
    
    if env_type == 'lineworld':
        max_steps = env_config.get('max_steps', 100)
        env = LineWorld(max_steps=max_steps)
        print(f"âœ… Environnement crÃ©Ã©: {env.env_name} (max_steps={max_steps})")
        return env
    else:
        raise ValueError(f"Type d'environnement non supportÃ©: {env_type}")


def create_algorithm(algo_config: dict, environment):
    """
    CrÃ©e l'algorithme selon la configuration.
    
    Args:
        algo_config: Configuration de l'algorithme
        environment: Environnement pour dimensionnement
        
    Returns:
        Instance de l'algorithme
    """
    algo_type = algo_config.get('type', 'q_learning').lower()
    
    if algo_type == 'q_learning':
        algorithm = QLearning.from_config(algo_config, environment)
        print(f"âœ… Algorithme crÃ©Ã©: {algorithm.algo_name}")
        print(f"   HyperparamÃ¨tres: Î±={algorithm.learning_rate}, Î³={algorithm.gamma}, Îµ={algorithm.epsilon}")
        return algorithm
    else:
        raise ValueError(f"Type d'algorithme non supportÃ©: {algo_type}")


def train_algorithm(algorithm, environment, training_config: dict):
    """
    EntraÃ®ne l'algorithme selon la configuration.
    
    Args:
        algorithm: Algorithme Ã  entraÃ®ner
        environment: Environnement d'entraÃ®nement
        training_config: Configuration d'entraÃ®nement
        
    Returns:
        RÃ©sultats d'entraÃ®nement
    """
    print(f"\nğŸš€ PHASE 1: ENTRAÃNEMENT AUTONOME")
    print("=" * 50)
    
    num_episodes = training_config.get('num_episodes', 1000)
    verbose = training_config.get('verbose', True)
    
    start_time = time.time()
    training_results = algorithm.train(
        environment=environment,
        num_episodes=num_episodes,
        verbose=verbose
    )
    training_time = time.time() - start_time
    
    print(f"\nâœ… EntraÃ®nement terminÃ© en {training_time:.2f}s")
    print(f"RÃ©compense finale: {training_results['final_reward']:.2f}")
    print(f"Ã‰pisodes entraÃ®nÃ©s: {training_results['episodes_trained']}")
    
    return training_results


def evaluate_agent(agent, eval_config: dict):
    """
    Ã‰value l'agent selon la configuration.
    
    Args:
        agent: Agent Ã  Ã©valuer
        eval_config: Configuration d'Ã©valuation
        
    Returns:
        RÃ©sultats d'Ã©valuation
    """
    print(f"\nğŸ“Š PHASE 2: Ã‰VALUATION POST-ENTRAÃNEMENT")
    print("=" * 50)
    
    num_episodes = eval_config.get('num_episodes', 100)
    verbose = eval_config.get('verbose', True)
    
    eval_results = agent.evaluate_performance(
        num_episodes=num_episodes,
        verbose=verbose
    )
    
    return eval_results


def demonstrate_agent(agent, demo_config: dict):
    """
    DÃ©monstration de l'agent selon la configuration.
    
    Args:
        agent: Agent Ã  dÃ©montrer
        demo_config: Configuration de dÃ©monstration
        
    Returns:
        RÃ©sultats de dÃ©monstration
    """
    print(f"\nğŸ¬ PHASE 3: DÃ‰MONSTRATION PAS-Ã€-PAS")
    print("=" * 50)
    
    num_episodes = demo_config.get('num_episodes', 1)
    show_q_values = demo_config.get('show_q_values', True)
    pause_between_steps = demo_config.get('pause_between_steps', False)
    
    demo_results = agent.demonstrate_step_by_step(
        num_episodes=num_episodes,
        show_q_values=show_q_values,
        pause_between_steps=pause_between_steps
    )
    
    return demo_results


def save_results(algorithm, agent, training_results, eval_results, demo_results, 
                output_config: dict, experiment_config: dict):
    """
    Sauvegarde tous les rÃ©sultats selon la configuration.
    
    Args:
        algorithm: Algorithme entraÃ®nÃ©
        agent: Agent wrapper
        training_results: RÃ©sultats d'entraÃ®nement
        eval_results: RÃ©sultats d'Ã©valuation  
        demo_results: RÃ©sultats de dÃ©monstration
        output_config: Configuration de sortie
        experiment_config: Configuration de l'expÃ©rience
    """
    if not output_config.get('save_results', True):
        print("\nğŸ’¾ Sauvegarde dÃ©sactivÃ©e dans la configuration")
        return
    
    print(f"\nğŸ’¾ PHASE 4: SAUVEGARDE DES RÃ‰SULTATS")
    print("=" * 50)
    
    # CrÃ©ation du rÃ©pertoire de sortie
    base_dir = output_config.get('base_dir', 'outputs/test_v4')
    os.makedirs(base_dir, exist_ok=True)
    
    # Sauvegarde du modÃ¨le (dÃ©jÃ  en JSON + pickle dans l'algorithme)
    if output_config.get('save_model', True):
        model_path = f"{base_dir}/model"
        if algorithm.save_model(model_path):
            print(f"âœ… ModÃ¨le sauvegardÃ©: {model_path}")
    
    # Sauvegarde des rÃ©sultats de l'agent (dÃ©jÃ  en JSON dans l'agent)
    agent_path = f"{base_dir}/agent"
    if agent.save_results(agent_path):
        print(f"âœ… RÃ©sultats agent sauvegardÃ©s: {agent_path}")
    
    # Sauvegarde du rÃ©sumÃ© simple en pickle (pas de problÃ¨me NumPy)
    summary_data = {
        "experiment": experiment_config,
        "training_results": training_results,
        "evaluation_results": eval_results,
        "demonstration_results": demo_results,
        "agent_summary": agent.get_performance_summary()
    }
    
    summary_path = f"{base_dir}/experiment_summary.pkl"
    try:
        with open(summary_path, 'wb') as f:
            pickle.dump(summary_data, f)
        print(f"âœ… RÃ©sumÃ© expÃ©rience sauvegardÃ©: {summary_path}")
    except Exception as e:
        print(f"âŒ Erreur sauvegarde rÃ©sumÃ©: {e}")


def display_final_summary(training_results, eval_results, demo_results):
    """
    Affiche le rÃ©sumÃ© final de l'expÃ©rience.
    
    Args:
        training_results: RÃ©sultats d'entraÃ®nement
        eval_results: RÃ©sultats d'Ã©valuation
        demo_results: RÃ©sultats de dÃ©monstration
    """
    print(f"\nğŸ‰ RÃ‰SUMÃ‰ FINAL DE L'EXPÃ‰RIENCE")
    print("=" * 50)
    
    print(f"ğŸ“ˆ ENTRAÃNEMENT:")
    print(f"   Ã‰pisodes: {training_results['episodes_trained']}")
    print(f"   RÃ©compense finale: {training_results['final_reward']:.2f}")
    
    print(f"\nğŸ“Š Ã‰VALUATION:")
    print(f"   Ã‰pisodes testÃ©s: {eval_results['num_episodes']}")
    print(f"   RÃ©compense moyenne: {eval_results['avg_reward']:.2f} Â± {eval_results['std_reward']:.2f}")
    print(f"   Taux de succÃ¨s: {eval_results['success_rate']:.1%}")
    print(f"   Longueur moyenne: {eval_results['avg_episode_length']:.1f} Ã©tapes")
    
    print(f"\nğŸ¬ DÃ‰MONSTRATION:")
    print(f"   Ã‰pisodes dÃ©montrÃ©s: {len(demo_results)}")
    for i, demo in enumerate(demo_results, 1):
        success_icon = "âœ…" if demo['success'] else "âŒ"
        print(f"   Ã‰pisode {i}: {demo['total_reward']:.2f} points, {len(demo['steps'])} Ã©tapes {success_icon}")
    
    # Ã‰valuation globale
    if eval_results['success_rate'] > 0.8 and eval_results['avg_reward'] > 5.0:
        print(f"\nğŸ† RÃ‰SULTAT: EXCELLENT! L'agent a trÃ¨s bien appris.")
    elif eval_results['success_rate'] > 0.5:
        print(f"\nğŸ‘ RÃ‰SULTAT: BON! L'agent a correctement appris.")
    else:
        print(f"\nâš ï¸ RÃ‰SULTAT: Ã€ amÃ©liorer. L'agent pourrait nÃ©cessiter plus d'entraÃ®nement.")


def main():
    """Fonction principale du test complet."""
    parser = argparse.ArgumentParser(description="Test complet du workflow RL")
    parser.add_argument('--config', '-c', default='experiments/configs/test_config.json',
                       help='Chemin vers le fichier de configuration JSON')
    args = parser.parse_args()
    
    print("ğŸ§ª TEST COMPLET DU WORKFLOW BASE - VERSION 4")
    print("=" * 60)
    print("ğŸ¯ Objectif: Valider LineWorld + Q-Learning + Agent")
    print("ğŸ“‹ Workflow: Config â†’ Environnement â†’ EntraÃ®nement â†’ Ã‰valuation â†’ DÃ©mo")
    print("=" * 60)
    
    try:
        # 1. Chargement de la configuration
        print(f"\nğŸ“ Ã‰TAPE 1: CHARGEMENT CONFIGURATION")
        config = load_config(args.config)
        print(f"ExpÃ©rience: {config['experiment']['name']}")
        print(f"Description: {config['experiment']['description']}")
        
        # 2. CrÃ©ation de l'environnement
        print(f"\nğŸŒ Ã‰TAPE 2: CRÃ‰ATION ENVIRONNEMENT")
        environment = create_environment(config['environment'])
        
        # Test rapide de l'environnement
        state = environment.reset()
        print(f"Ã‰tat initial: {state}")
        environment.render('console')
        
        # 3. CrÃ©ation de l'algorithme
        print(f"\nğŸ§  Ã‰TAPE 3: CRÃ‰ATION ALGORITHME")
        algorithm = create_algorithm(config['algorithm'], environment)
        
        # 4. EntraÃ®nement autonome
        training_results = train_algorithm(algorithm, environment, config['training'])
        
        # 5. CrÃ©ation de l'agent wrapper
        print(f"\nğŸ¤– Ã‰TAPE 4: CRÃ‰ATION AGENT WRAPPER")
        agent = Agent(algorithm, environment, f"TestAgent_{config['experiment']['name']}")
        
        # 6. Ã‰valuation
        eval_results = evaluate_agent(agent, config['evaluation'])
        
        # 7. DÃ©monstration
        demo_results = demonstrate_agent(agent, config['demonstration'])
        
        # 8. Sauvegarde
        save_results(algorithm, agent, training_results, eval_results, demo_results,
                    config['outputs'], config['experiment'])
        
        # 9. RÃ©sumÃ© final
        display_final_summary(training_results, eval_results, demo_results)
        
        print(f"\nâœ… TEST COMPLET RÃ‰USSI!")
        print("ğŸ“ L'architecture de base est validÃ©e et fonctionnelle.")
        print("ğŸš€ PrÃªt pour l'extension avec d'autres algorithmes et environnements!")
        
    except KeyboardInterrupt:
        print(f"\n\nâ¸ï¸ Test interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n\nğŸ’¥ Erreur lors du test: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()